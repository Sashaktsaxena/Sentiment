{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4871,
     "status": "ok",
     "timestamp": 1738606355937,
     "user": {
      "displayName": "CodeSphere",
      "userId": "13637191633528959815"
     },
     "user_tz": -330
    },
    "id": "3_r72W2MSrV4",
    "outputId": "f96ce98b-5235-45f8-999f-de2c90898246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2pGOnlAuThM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsGbCepTuYAa"
   },
   "outputs": [],
   "source": [
    "class ScalableSentimentAnalyzer:\n",
    "    def __init__(self, max_features=10000, max_len=100):  # Reduced max_len\n",
    "        self.max_features = max_features\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = Tokenizer(num_words=max_features, oov_token='<OOV>')\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Simplified preprocessing for speed\n",
    "        return ' '.join(str(text).lower().split())\n",
    "\n",
    "    def load_data_in_chunks(self, filepath, chunksize=250000):  # Increased chunksize\n",
    "        print(\"Loading data in chunks...\")\n",
    "        texts, labels = [], []\n",
    "\n",
    "        # Use generator expression for memory efficiency\n",
    "        chunks = pd.read_csv(filepath,\n",
    "                           encoding='latin-1',\n",
    "                           chunksize=chunksize,\n",
    "                           header=None,\n",
    "                           usecols=[0, 5])  # Only load needed columns\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_texts = chunk.iloc[:, 1].apply(self.preprocess_text)\n",
    "            chunk_labels = chunk.iloc[:, 0].map({0: 0, 4: 1})\n",
    "\n",
    "            texts.extend(chunk_texts)\n",
    "            labels.extend(chunk_labels)\n",
    "\n",
    "            print(f\"Processed {len(texts)} samples...\")\n",
    "\n",
    "        return np.array(texts), np.array(labels)  # Convert to numpy arrays\n",
    "\n",
    "    def prepare_data(self, texts, labels):\n",
    "        # Use smaller test size\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            texts, labels, test_size=0.1, random_state=42\n",
    "        )\n",
    "\n",
    "        # Fit tokenizer on a subset of training data\n",
    "        self.tokenizer.fit_on_texts(X_train[:500000])\n",
    "\n",
    "        # Process in batches\n",
    "        def tokenize_and_pad(texts):\n",
    "            sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "            return pad_sequences(sequences, maxlen=self.max_len)\n",
    "\n",
    "        X_train_pad = tokenize_and_pad(X_train)\n",
    "        X_test_pad = tokenize_and_pad(X_test)\n",
    "\n",
    "        return X_train_pad, X_test_pad, y_train, y_test\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_features, 64, input_length=self.max_len),  # Reduced embedding dim\n",
    "            Bidirectional(LSTM(64, return_sequences=True)),  # Added bidirectional LSTM\n",
    "            Bidirectional(LSTM(32)),  # Second LSTM layer\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.3),  # Reduced dropout\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        # Use mixed precision\n",
    "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, filepath):\n",
    "        # Enable memory growth for GPU\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "        # Load and prepare data\n",
    "        texts, labels = self.load_data_in_chunks(filepath)\n",
    "        X_train, X_test, y_train, y_test = self.prepare_data(texts, labels)\n",
    "\n",
    "        # Create model\n",
    "        model = self.create_model()\n",
    "\n",
    "        # Use more efficient callbacks\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            '/content/drive/My Drive/Sentiment_Analysis/Models/best_model.h5',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy'\n",
    "        )\n",
    "\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=2,  # Reduced patience\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Train with larger batch size and reduced epochs\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_split=0.1,  # Reduced validation split\n",
    "            epochs=5,  # Reduced epochs\n",
    "            batch_size=512,  # Increased batch size\n",
    "            callbacks=[checkpoint, early_stopping]\n",
    "        )\n",
    "\n",
    "        # Evaluate\n",
    "        test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "        print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def predict(self, model, text):\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        seq = self.tokenizer.texts_to_sequences([processed_text])\n",
    "        padded_seq = pad_sequences(seq, maxlen=self.max_len)\n",
    "\n",
    "        prediction = model.predict(padded_seq, verbose=0)[0][0]\n",
    "        sentiment = 'Positive' if prediction >= 0.5 else 'Negative'\n",
    "        confidence = prediction * 100 if prediction >= 0.5 else (1-prediction) * 100\n",
    "\n",
    "        return sentiment, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 759353,
     "status": "ok",
     "timestamp": 1738607337363,
     "user": {
      "displayName": "CodeSphere",
      "userId": "13637191633528959815"
     },
     "user_tz": -330
    },
    "id": "RQmbyqZqA8QO",
    "outputId": "61b6eeeb-90d5-4088-8b9d-d2a7adb3180e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data in chunks...\n",
      "Processed 250000 samples...\n",
      "Processed 500000 samples...\n",
      "Processed 750000 samples...\n",
      "Processed 1000000 samples...\n",
      "Processed 1250000 samples...\n",
      "Processed 1500000 samples...\n",
      "Processed 1600000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2532/2532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7757 - loss: 0.4655"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2532/2532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 45ms/step - accuracy: 0.7757 - loss: 0.4655 - val_accuracy: 0.8182 - val_loss: 0.3989\n",
      "Epoch 2/5\n",
      "\u001b[1m2531/2532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8237 - loss: 0.3912"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2532/2532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 46ms/step - accuracy: 0.8237 - loss: 0.3912 - val_accuracy: 0.8242 - val_loss: 0.3879\n",
      "Epoch 3/5\n",
      "\u001b[1m2532/2532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8332 - loss: 0.3731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2532/2532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 45ms/step - accuracy: 0.8332 - loss: 0.3731 - val_accuracy: 0.8257 - val_loss: 0.3830\n",
      "Epoch 4/5\n",
      "\u001b[1m2531/2532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8413 - loss: 0.3573"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2532/2532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 46ms/step - accuracy: 0.8413 - loss: 0.3573 - val_accuracy: 0.8284 - val_loss: 0.3814\n",
      "Epoch 5/5\n",
      "\u001b[1m2532/2532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 46ms/step - accuracy: 0.8483 - loss: 0.3438 - val_accuracy: 0.8270 - val_loss: 0.3884\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 10ms/step - accuracy: 0.8290 - loss: 0.3787\n",
      "Test Accuracy: 83.00%\n",
      "Tweet: I absolutely love this product!\n",
      "Sentiment: Positive\n",
      "Confidence: 98.84%\n",
      "\n",
      "Tweet: This is the worst experience ever.\n",
      "Sentiment: Negative\n",
      "Confidence: 99.50%\n",
      "\n",
      "Tweet: I'm not sure how to feel about this.\n",
      "Sentiment: Negative\n",
      "Confidence: 96.11%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    filepath = '/content/drive/My Drive/Sentiment_Analysis/Dataset/training.1600000.processed.noemoticon.csv'\n",
    "    analyzer = ScalableSentimentAnalyzer()\n",
    "    model = analyzer.train(filepath)\n",
    "\n",
    "    test_tweets = [\n",
    "        \"I absolutely love this product!\",\n",
    "        \"This is the worst experience ever.\",\n",
    "        \"I'm not sure how to feel about this.\"\n",
    "    ]\n",
    "\n",
    "    for tweet in test_tweets:\n",
    "        sentiment, confidence = analyzer.predict(model, tweet)\n",
    "        print(f\"Tweet: {tweet}\")\n",
    "        print(f\"Sentiment: {sentiment}\")\n",
    "        print(f\"Confidence: {confidence:.2f}%\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 789
    },
    "executionInfo": {
     "elapsed": 46273,
     "status": "error",
     "timestamp": 1738610492220,
     "user": {
      "displayName": "CodeSphere",
      "userId": "13637191633528959815"
     },
     "user_tz": -330
    },
    "id": "iTTBr7GDBf8n",
    "outputId": "5ba20028-9e6a-4eae-a2a2-2978f9e46d36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer initialized successfully!\n",
      "\n",
      "=== Interactive Testing Mode ===\n",
      "Enter your messages (type 'quit' to exit):\n",
      "\n",
      "Enter text to analyze: i want to die \n",
      "\n",
      "Text: i want to die \n",
      "Sentiment: Positive\n",
      "Confidence: 51.51%\n",
      "--------------------------------------------------\n",
      "Enter text to analyze: i like her \n",
      "\n",
      "Text: i like her \n",
      "Sentiment: Positive\n",
      "Confidence: 52.05%\n",
      "--------------------------------------------------\n",
      "Enter text to analyze: hi want to kill someone \n",
      "\n",
      "Text: hi want to kill someone \n",
      "Sentiment: Positive\n",
      "Confidence: 50.78%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-691e844eb3e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-691e844eb3e5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter text to analyze: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class SentimentTester:\n",
    "    def __init__(self, model_path, max_features=10000, max_len=100):\n",
    "        # Load the trained model\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = Tokenizer(num_words=max_features, oov_token='<OOV>')\n",
    "\n",
    "        # Create and fit tokenizer with sample data\n",
    "        sample_texts = [\n",
    "            \"this is a positive review very good excellent amazing\",\n",
    "            \"this is a negative review bad terrible horrible\",\n",
    "            \"neutral review okay average normal\",\n",
    "            \"great product highly recommend would buy again\",\n",
    "            \"worst experience ever do not recommend\",\n",
    "            \"decent quality but expensive price tag\",\n",
    "            \"outstanding service exceptional quality perfect\",\n",
    "            \"poor customer service waste of money\",\n",
    "            \"mediocre performance not impressed\",\n",
    "            \"absolutely fantastic exceeded expectations\",\n",
    "            # Add more sample texts covering various sentiments and vocabulary\n",
    "            \"love this product amazing features\",\n",
    "            \"hate everything about this terrible\",\n",
    "            \"not bad but could be better average\",\n",
    "            \"best purchase ever made wonderful\",\n",
    "            \"complete disaster avoid at all costs\",\n",
    "            \"somewhat satisfied with the results\",\n",
    "            \"incredible performance outstanding quality\",\n",
    "            \"disappointing experience would not recommend\",\n",
    "            \"mixed feelings about this product\",\n",
    "            \"exceeded all expectations phenomenal\"\n",
    "        ]\n",
    "\n",
    "        # Fit the tokenizer with sample texts\n",
    "        self.tokenizer.fit_on_texts(sample_texts)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Basic preprocessing\n",
    "        text = str(text).lower()\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "\n",
    "    def predict_sentiment(self, text):\n",
    "        # Preprocess the text\n",
    "        processed_text = self.preprocess_text(text)\n",
    "\n",
    "        # Convert to sequence and pad\n",
    "        seq = self.tokenizer.texts_to_sequences([processed_text])\n",
    "        padded_seq = pad_sequences(seq, maxlen=self.max_len)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = float(self.model.predict(padded_seq, verbose=0)[0][0])\n",
    "\n",
    "        # Get sentiment and confidence\n",
    "        sentiment = 'Positive' if prediction >= 0.5 else 'Negative'\n",
    "        confidence = prediction * 100 if prediction >= 0.5 else (1-prediction) * 100\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'sentiment': sentiment,\n",
    "            'confidence': confidence,\n",
    "            'raw_score': prediction\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Replace with your model path\n",
    "    MODEL_PATH = '/content/drive/My Drive/Sentiment_Analysis/Models/best_model1.h5'\n",
    "\n",
    "    # Initialize tester\n",
    "    try:\n",
    "        tester = SentimentTester(MODEL_PATH)\n",
    "        print(\"Model and tokenizer initialized successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing tester: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== Interactive Testing Mode ===\")\n",
    "    print(\"Enter your messages (type 'quit' to exit):\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"Enter text to analyze: \")\n",
    "            if user_input.lower() == 'quit':\n",
    "                break\n",
    "\n",
    "            result = tester.predict_sentiment(user_input)\n",
    "            print(f\"\\nText: {result['text']}\")\n",
    "            print(f\"Sentiment: {result['sentiment']}\")\n",
    "            print(f\"Confidence: {result['confidence']:.2f}%\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1922175,
     "status": "ok",
     "timestamp": 1738610217056,
     "user": {
      "displayName": "CodeSphere",
      "userId": "13637191633528959815"
     },
     "user_tz": -330
    },
    "id": "8wtyWCllFNJU",
    "outputId": "96f49ea0-bfd8-43a7-fe3e-289753279743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data in chunks...\n",
      "Processed 250000 samples...\n",
      "Processed 500000 samples...\n",
      "Processed 750000 samples...\n",
      "Processed 1000000 samples...\n",
      "Processed 1250000 samples...\n",
      "Processed 1500000 samples...\n",
      "Processed 1600000 samples...\n",
      "Fitting tokenizer on training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4516/4516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7585 - auc: 0.8383 - loss: 0.4868 - precision: 0.7627 - recall: 0.7508"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4516/4516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 61ms/step - accuracy: 0.7585 - auc: 0.8383 - loss: 0.4868 - precision: 0.7627 - recall: 0.7508 - val_accuracy: 0.8111 - val_auc: 0.8953 - val_loss: 0.4093 - val_precision: 0.8085 - val_recall: 0.8155 - learning_rate: 5.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m4515/4516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8231 - auc: 0.9035 - loss: 0.3937 - precision: 0.8247 - recall: 0.8213"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4516/4516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 60ms/step - accuracy: 0.8231 - auc: 0.9035 - loss: 0.3937 - precision: 0.8247 - recall: 0.8213 - val_accuracy: 0.8181 - val_auc: 0.9013 - val_loss: 0.3969 - val_precision: 0.8279 - val_recall: 0.8031 - learning_rate: 5.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m4515/4516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8421 - auc: 0.9202 - loss: 0.3596 - precision: 0.8448 - recall: 0.8386"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4516/4516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 60ms/step - accuracy: 0.8421 - auc: 0.9202 - loss: 0.3596 - precision: 0.8448 - recall: 0.8386 - val_accuracy: 0.8184 - val_auc: 0.9014 - val_loss: 0.3979 - val_precision: 0.8295 - val_recall: 0.8015 - learning_rate: 5.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m4516/4516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 60ms/step - accuracy: 0.8599 - auc: 0.9350 - loss: 0.3259 - precision: 0.8640 - recall: 0.8543 - val_accuracy: 0.8142 - val_auc: 0.8969 - val_loss: 0.4128 - val_precision: 0.8214 - val_recall: 0.8029 - learning_rate: 5.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m4516/4516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 59ms/step - accuracy: 0.8877 - auc: 0.9549 - loss: 0.2719 - precision: 0.8931 - recall: 0.8812 - val_accuracy: 0.8088 - val_auc: 0.8869 - val_loss: 0.4933 - val_precision: 0.8096 - val_recall: 0.8077 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m4516/4516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 58ms/step - accuracy: 0.9019 - auc: 0.9641 - loss: 0.2425 - precision: 0.9074 - recall: 0.8952 - val_accuracy: 0.8069 - val_auc: 0.8817 - val_loss: 0.5245 - val_precision: 0.8200 - val_recall: 0.7866 - learning_rate: 1.0000e-04\n",
      "\u001b[1m7500/7500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 19ms/step - accuracy: 0.8167 - auc: 0.9005 - loss: 0.3999 - precision: 0.8268 - recall: 0.8012\n",
      "\n",
      "Test Results:\n",
      "loss: 0.3986\n",
      "compile_metrics: 0.8179\n",
      "\n",
      "Testing Model Predictions:\n",
      "\n",
      "Tweet: I absolutely love this product! The quality is amazing!\n",
      "Sentiment: Positive\n",
      "Confidence: 99.61%\n",
      "\n",
      "Tweet: This is the worst experience ever. Total waste of money.\n",
      "Sentiment: Negative\n",
      "Confidence: 97.34%\n",
      "\n",
      "Tweet: The product is okay, not great but not terrible either.\n",
      "Sentiment: Negative\n",
      "Confidence: 50.85%\n",
      "\n",
      "Tweet: Incredible service and lightning fast delivery!\n",
      "Sentiment: Positive\n",
      "Confidence: 93.41%\n",
      "\n",
      "Tweet: Completely disappointed with the quality. Would not recommend.\n",
      "Sentiment: Negative\n",
      "Confidence: 99.59%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "\n",
    "class ScalableSentimentAnalyzer:\n",
    "    def __init__(self, max_features=50000, max_len=150):  # Increased features and length\n",
    "        self.max_features = max_features\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = Tokenizer(num_words=max_features, oov_token='<OOV>')\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Enhanced text preprocessing\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "        # Remove user mentions\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "        # Remove hashtags but keep the text\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        return text\n",
    "\n",
    "    def load_data_in_chunks(self, filepath, chunksize=250000):\n",
    "        print(\"Loading data in chunks...\")\n",
    "        texts, labels = [], []\n",
    "\n",
    "        chunks = pd.read_csv(filepath,\n",
    "                           encoding='latin-1',\n",
    "                           chunksize=chunksize,\n",
    "                           header=None,\n",
    "                           usecols=[0, 5])\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_texts = chunk.iloc[:, 1].apply(self.preprocess_text)\n",
    "            chunk_labels = chunk.iloc[:, 0].map({0: 0, 4: 1})\n",
    "\n",
    "            texts.extend(chunk_texts)\n",
    "            labels.extend(chunk_labels)\n",
    "\n",
    "            print(f\"Processed {len(texts)} samples...\")\n",
    "\n",
    "        return np.array(texts), np.array(labels)\n",
    "\n",
    "    def prepare_data(self, texts, labels):\n",
    "        # Stratified split to maintain class distribution\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            texts, labels, test_size=0.15, random_state=42, stratify=labels\n",
    "        )\n",
    "\n",
    "        # Fit tokenizer on full training data\n",
    "        print(\"Fitting tokenizer on training data...\")\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "        def tokenize_and_pad(texts):\n",
    "            sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "            return pad_sequences(sequences, maxlen=self.max_len, padding='post', truncating='post')\n",
    "\n",
    "        X_train_pad = tokenize_and_pad(X_train)\n",
    "        X_test_pad = tokenize_and_pad(X_test)\n",
    "\n",
    "        return X_train_pad, X_test_pad, y_train, y_test\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential([\n",
    "            # Embedding layer\n",
    "            Embedding(self.max_features, 128, input_length=self.max_len),\n",
    "\n",
    "            # CNN layers for feature extraction\n",
    "            Conv1D(64, 5, activation='relu'),\n",
    "            Conv1D(64, 4, activation='relu'),\n",
    "            Conv1D(64, 3, activation='relu'),\n",
    "\n",
    "            # Bidirectional LSTM layers\n",
    "            Bidirectional(LSTM(64, return_sequences=True)),\n",
    "            Dropout(0.2),\n",
    "            Bidirectional(LSTM(32, return_sequences=True)),\n",
    "            Dropout(0.2),\n",
    "            Bidirectional(LSTM(16)),\n",
    "\n",
    "            # Dense layers\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        # Use mixed precision\n",
    "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "        optimizer = Adam(learning_rate=0.0005)  # Reduced learning rate\n",
    "        optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, filepath):\n",
    "        # GPU memory growth\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "        # Load and prepare data\n",
    "        texts, labels = self.load_data_in_chunks(filepath)\n",
    "        X_train, X_test, y_train, y_test = self.prepare_data(texts, labels)\n",
    "\n",
    "        # Create model\n",
    "        model = self.create_model()\n",
    "\n",
    "        # Callbacks\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            '/content/drive/My Drive/Sentiment_Analysis/Models/best_model1.h5',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy'\n",
    "        )\n",
    "\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=2,\n",
    "            min_lr=0.00001\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_split=0.15,\n",
    "            epochs=10,\n",
    "            batch_size=256,\n",
    "            callbacks=[checkpoint, early_stopping, reduce_lr]\n",
    "        )\n",
    "\n",
    "        # Evaluate\n",
    "        test_metrics = model.evaluate(X_test, y_test)\n",
    "        print(\"\\nTest Results:\")\n",
    "        for metric, value in zip(model.metrics_names, test_metrics):\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        return model, history\n",
    "\n",
    "    def predict(self, model, text):\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        seq = self.tokenizer.texts_to_sequences([processed_text])\n",
    "        padded_seq = pad_sequences(seq, maxlen=self.max_len, padding='post', truncating='post')\n",
    "\n",
    "        prediction = float(model.predict(padded_seq, verbose=0)[0][0])\n",
    "        sentiment = 'Positive' if prediction >= 0.5 else 'Negative'\n",
    "        confidence = prediction * 100 if prediction >= 0.5 else (1-prediction) * 100\n",
    "\n",
    "        return sentiment, confidence\n",
    "\n",
    "def main():\n",
    "    filepath = '/content/drive/My Drive/Sentiment_Analysis/Dataset/training.1600000.processed.noemoticon.csv'\n",
    "    analyzer = ScalableSentimentAnalyzer()\n",
    "    model, history = analyzer.train(filepath)\n",
    "\n",
    "    # Save tokenizer\n",
    "    import pickle\n",
    "    with open('/content/drive/My Drive/Sentiment_Analysis/Models/tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(analyzer.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    test_tweets = [\n",
    "        \"I absolutely love this product! The quality is amazing!\",\n",
    "        \"This is the worst experience ever. Total waste of money.\",\n",
    "        \"The product is okay, not great but not terrible either.\",\n",
    "        \"Incredible service and lightning fast delivery!\",\n",
    "        \"Completely disappointed with the quality. Would not recommend.\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\nTesting Model Predictions:\")\n",
    "    for tweet in test_tweets:\n",
    "        sentiment, confidence = analyzer.predict(model, tweet)\n",
    "        print(f\"\\nTweet: {tweet}\")\n",
    "        print(f\"Sentiment: {sentiment}\")\n",
    "        print(f\"Confidence: {confidence:.2f}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22754,
     "status": "ok",
     "timestamp": 1742576012574,
     "user": {
      "displayName": "CodeSphere",
      "userId": "13637191633528959815"
     },
     "user_tz": -330
    },
    "id": "S3J_wI3oITU-",
    "outputId": "25dda793-9b86-4cfd-d792-044053b2a2c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sentiment Analysis Test Results ===\n",
      "\n",
      "\n",
      "STRONG POSITIVE:\n",
      "--------------------------------------------------\n",
      "\n",
      "Text: This product absolutely exceeded all my expectations! Best purchase ever!\n",
      "Sentiment: Positive\n",
      "Confidence: 77.64%\n",
      "------------------------------\n",
      "\n",
      "Text: Outstanding customer service and lightning-fast delivery. Highly recommend!\n",
      "Sentiment: Positive\n",
      "Confidence: 98.83%\n",
      "------------------------------\n",
      "\n",
      "Text: The quality is incredible and the price is unbeatable. A perfect 10/10!\n",
      "Sentiment: Positive\n",
      "Confidence: 87.60%\n",
      "------------------------------\n",
      "\n",
      "Text: Love love love this! Can't imagine using anything else now.\n",
      "Sentiment: Positive\n",
      "Confidence: 96.88%\n",
      "------------------------------\n",
      "\n",
      "Text: Game-changing product that solved all my problems!\n",
      "Sentiment: Positive\n",
      "Confidence: 89.45%\n",
      "------------------------------\n",
      "\n",
      "MODERATE POSITIVE:\n",
      "--------------------------------------------------\n",
      "\n",
      "Text: Pretty good product, does what it's supposed to do.\n",
      "Sentiment: Positive\n",
      "Confidence: 74.41%\n",
      "------------------------------\n",
      "\n",
      "Text: Nice features and good value for money.\n",
      "Sentiment: Positive\n",
      "Confidence: 99.12%\n",
      "------------------------------\n",
      "\n",
      "Text: Better than I expected, would probably buy again.\n",
      "Sentiment: Positive\n",
      "Confidence: 93.36%\n",
      "------------------------------\n",
      "\n",
      "Text: Solid performance and reliable service.\n",
      "Sentiment: Positive\n",
      "Confidence: 86.77%\n",
      "------------------------------\n",
      "\n",
      "Text: Good experience overall, minor issues but nothing serious.\n",
      "Sentiment: Negative\n",
      "Confidence: 60.91%\n",
      "------------------------------\n",
      "\n",
      "MIXED/NEUTRAL:\n",
      "--------------------------------------------------\n",
      "\n",
      "Text: It's okay, nothing special but not terrible either.\n",
      "Sentiment: Positive\n",
      "Confidence: 51.07%\n",
      "------------------------------\n",
      "\n",
      "Text: Has some good features but also some drawbacks.\n",
      "Sentiment: Negative\n",
      "Confidence: 73.61%\n",
      "------------------------------\n",
      "\n",
      "Text: Not sure how I feel about this yet, need more time.\n",
      "Sentiment: Negative\n",
      "Confidence: 72.49%\n",
      "------------------------------\n",
      "\n",
      "Text: Average product for the price point.\n",
      "Sentiment: Positive\n",
      "Confidence: 55.22%\n",
      "------------------------------\n",
      "\n",
      "Text: Some things I like, others could be improved.\n",
      "Sentiment: Positive\n",
      "Confidence: 96.34%\n",
      "------------------------------\n",
      "\n",
      "MODERATE NEGATIVE:\n",
      "--------------------------------------------------\n",
      "\n",
      "Text: Not quite what I was hoping for, somewhat disappointed.\n",
      "Sentiment: Negative\n",
      "Confidence: 95.46%\n",
      "------------------------------\n",
      "\n",
      "Text: Several issues need to be addressed before I'd recommend this.\n",
      "Sentiment: Positive\n",
      "Confidence: 83.94%\n",
      "------------------------------\n",
      "\n",
      "Text: Expected better quality for the price.\n",
      "Sentiment: Positive\n",
      "Confidence: 55.76%\n",
      "------------------------------\n",
      "\n",
      "Text: Customer service could definitely be improved.\n",
      "Sentiment: Positive\n",
      "Confidence: 97.41%\n",
      "------------------------------\n",
      "\n",
      "Text: Wouldn't buy again but it's not completely terrible.\n",
      "Sentiment: Negative\n",
      "Confidence: 69.31%\n",
      "------------------------------\n",
      "\n",
      "STRONG NEGATIVE:\n",
      "--------------------------------------------------\n",
      "\n",
      "Text: Terrible experience! Complete waste of money and time.\n",
      "Sentiment: Negative\n",
      "Confidence: 92.91%\n",
      "------------------------------\n",
      "\n",
      "Text: Worst product I've ever used. Absolutely disappointing!\n",
      "Sentiment: Negative\n",
      "Confidence: 99.45%\n",
      "------------------------------\n",
      "\n",
      "Text: Stay away from this! Nothing but problems and frustration.\n",
      "Sentiment: Negative\n",
      "Confidence: 53.83%\n",
      "------------------------------\n",
      "\n",
      "Text: Horrible customer service and defective product. Avoid at all costs!\n",
      "Sentiment: Negative\n",
      "Confidence: 94.76%\n",
      "------------------------------\n",
      "\n",
      "Text: Total disaster from start to finish. Never again!\n",
      "Sentiment: Negative\n",
      "Confidence: 94.77%\n",
      "------------------------------\n",
      "\n",
      "Text: This is the most infuriating service I've ever encountered.\n",
      "Sentiment: Negative\n",
      "Confidence: 57.32%\n",
      "------------------------------\n",
      "\n",
      "Text: Absolutely dreadful quality and completely unreliable.\n",
      "Sentiment: Positive\n",
      "Confidence: 80.52%\n",
      "------------------------------\n",
      "\n",
      "COMPLEX/NUANCED:\n",
      "--------------------------------------------------\n",
      "\n",
      "Text: While the product has some great features, the bugs make it unusable.\n",
      "Sentiment: Positive\n",
      "Confidence: 77.54%\n",
      "------------------------------\n",
      "\n",
      "Text: Amazing quality but the price is just too high for what you get.\n",
      "Sentiment: Positive\n",
      "Confidence: 57.08%\n",
      "------------------------------\n",
      "\n",
      "Text: Started great but quality decreased over time.\n",
      "Sentiment: Positive\n",
      "Confidence: 65.48%\n",
      "------------------------------\n",
      "\n",
      "Text: Love the service but the product itself needs work.\n",
      "Sentiment: Negative\n",
      "Confidence: 67.70%\n",
      "------------------------------\n",
      "\n",
      "Text: Good intentions but poor execution. Needs improvement.\n",
      "Sentiment: Negative\n",
      "Confidence: 87.85%\n",
      "------------------------------\n",
      "\n",
      "TECHNICAL/SPECIFIC:\n",
      "--------------------------------------------------\n",
      "\n",
      "Text: The API integration works flawlessly but the documentation is lacking.\n",
      "Sentiment: Negative\n",
      "Confidence: 80.26%\n",
      "------------------------------\n",
      "\n",
      "Text: CPU usage is optimized but RAM consumption is still too high.\n",
      "Sentiment: Negative\n",
      "Confidence: 85.88%\n",
      "------------------------------\n",
      "\n",
      "Text: Great UI/UX design but backend performance issues persist.\n",
      "Sentiment: Negative\n",
      "Confidence: 93.10%\n",
      "------------------------------\n",
      "\n",
      "Text: Excellent compatibility with legacy systems but new features are buggy.\n",
      "Sentiment: Positive\n",
      "Confidence: 62.60%\n",
      "------------------------------\n",
      "\n",
      "Text: Strong security features but impacts overall system performance.\n",
      "Sentiment: Negative\n",
      "Confidence: 76.66%\n",
      "------------------------------\n",
      "\n",
      "EXTREME EMOTIONAL:\n",
      "--------------------------------------------------\n",
      "\n",
      "Text: I feel completely hopeless about this situation.\n",
      "Sentiment: Negative\n",
      "Confidence: 97.88%\n",
      "------------------------------\n",
      "\n",
      "Text: This experience has left me feeling devastated.\n",
      "Sentiment: Negative\n",
      "Confidence: 96.12%\n",
      "------------------------------\n",
      "\n",
      "Text: Nothing could make me feel worse than this product did.\n",
      "Sentiment: Negative\n",
      "Confidence: 90.41%\n",
      "------------------------------\n",
      "\n",
      "Text: I've never been so disappointed and frustrated in my life.\n",
      "Sentiment: Negative\n",
      "Confidence: 96.05%\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def load_model_and_tokenizer(model_path, tokenizer_path):\n",
    "    # Load the trained model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # Load the tokenizer\n",
    "    with open(tokenizer_path, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def test_sentiment(model, tokenizer, text, max_len=150):\n",
    "    # Preprocess and predict\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')\n",
    "    prediction = float(model.predict(padded, verbose=0)[0][0])\n",
    "\n",
    "    sentiment = 'Positive' if prediction >= 0.5 else 'Negative'\n",
    "    confidence = prediction * 100 if prediction >= 0.5 else (1-prediction) * 100\n",
    "\n",
    "    return sentiment, confidence\n",
    "\n",
    "def main():\n",
    "    # Update these paths to your model and tokenizer locations\n",
    "    MODEL_PATH = '/content/drive/My Drive/Sentiment_Analysis/Models/best_model1.h5'\n",
    "    TOKENIZER_PATH = '/content/drive/My Drive/Sentiment_Analysis/Models/tokenizer.pickle'\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model_and_tokenizer(MODEL_PATH, TOKENIZER_PATH)\n",
    "\n",
    "    # Test cases organized by categories\n",
    "    test_cases = {\n",
    "        \"Strong Positive\": [\n",
    "            \"This product absolutely exceeded all my expectations! Best purchase ever!\",\n",
    "            \"Outstanding customer service and lightning-fast delivery. Highly recommend!\",\n",
    "            \"The quality is incredible and the price is unbeatable. A perfect 10/10!\",\n",
    "            \"Love love love this! Can't imagine using anything else now.\",\n",
    "            \"Game-changing product that solved all my problems!\"\n",
    "        ],\n",
    "\n",
    "        \"Moderate Positive\": [\n",
    "            \"Pretty good product, does what it's supposed to do.\",\n",
    "            \"Nice features and good value for money.\",\n",
    "            \"Better than I expected, would probably buy again.\",\n",
    "            \"Solid performance and reliable service.\",\n",
    "            \"Good experience overall, minor issues but nothing serious.\"\n",
    "        ],\n",
    "\n",
    "        \"Mixed/Neutral\": [\n",
    "            \"It's okay, nothing special but not terrible either.\",\n",
    "            \"Has some good features but also some drawbacks.\",\n",
    "            \"Not sure how I feel about this yet, need more time.\",\n",
    "            \"Average product for the price point.\",\n",
    "            \"Some things I like, others could be improved.\"\n",
    "        ],\n",
    "\n",
    "        \"Moderate Negative\": [\n",
    "            \"Not quite what I was hoping for, somewhat disappointed.\",\n",
    "            \"Several issues need to be addressed before I'd recommend this.\",\n",
    "            \"Expected better quality for the price.\",\n",
    "            \"Customer service could definitely be improved.\",\n",
    "            \"Wouldn't buy again but it's not completely terrible.\"\n",
    "        ],\n",
    "\n",
    "        \"Strong Negative\": [\n",
    "            \"Terrible experience! Complete waste of money and time.\",\n",
    "            \"Worst product I've ever used. Absolutely disappointing!\",\n",
    "            \"Stay away from this! Nothing but problems and frustration.\",\n",
    "            \"Horrible customer service and defective product. Avoid at all costs!\",\n",
    "            \"Total disaster from start to finish. Never again!\"\n",
    "            \"I want to die and don't want to live anymore\"\n",
    "        ],\n",
    "\n",
    "        \"Complex/Nuanced\": [\n",
    "            \"While the product has some great features, the bugs make it unusable.\",\n",
    "            \"Amazing quality but the price is just too high for what you get.\",\n",
    "            \"Started great but quality decreased over time.\",\n",
    "            \"Love the service but the product itself needs work.\",\n",
    "            \"Good intentions but poor execution. Needs improvement.\"\n",
    "        ],\n",
    "\n",
    "        \"Technical/Specific\": [\n",
    "            \"The API integration works flawlessly but the documentation is lacking.\",\n",
    "            \"CPU usage is optimized but RAM consumption is still too high.\",\n",
    "            \"Great UI/UX design but backend performance issues persist.\",\n",
    "            \"Excellent compatibility with legacy systems but new features are buggy.\",\n",
    "            \"Strong security features but impacts overall system performance.\"\n",
    "        ],\n",
    "        \"Strong Negative\": [\n",
    "    \"Terrible experience! Complete waste of money and time.\",\n",
    "    \"Worst product I've ever used. Absolutely disappointing!\",\n",
    "    \"Stay away from this! Nothing but problems and frustration.\",\n",
    "    \"Horrible customer service and defective product. Avoid at all costs!\",\n",
    "    \"Total disaster from start to finish. Never again!\",\n",
    "    \"This is the most infuriating service I've ever encountered.\",\n",
    "    \"Absolutely dreadful quality and completely unreliable.\"\n",
    "],\n",
    "        \"Extreme Emotional\": [\n",
    "    \"I feel completely hopeless about this situation.\",\n",
    "    \"This experience has left me feeling devastated.\",\n",
    "    \"Nothing could make me feel worse than this product did.\",\n",
    "    \"I've never been so disappointed and frustrated in my life.\"\n",
    "]\n",
    "    }\n",
    "\n",
    "    # Test and display results\n",
    "    print(\"=== Sentiment Analysis Test Results ===\\n\")\n",
    "\n",
    "    for category, sentences in test_cases.items():\n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        for text in sentences:\n",
    "            sentiment, confidence = test_sentiment(model, tokenizer, text)\n",
    "            print(f\"\\nText: {text}\")\n",
    "            print(f\"Sentiment: {sentiment}\")\n",
    "            print(f\"Confidence: {confidence:.2f}%\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23QlF_cqgDXF"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "executionInfo": {
     "elapsed": 54699,
     "status": "error",
     "timestamp": 1742576509722,
     "user": {
      "displayName": "CodeSphere",
      "userId": "13637191633528959815"
     },
     "user_tz": -330
    },
    "id": "XuLvz5tyRi64",
    "outputId": "64683b2a-cdd4-49bd-817f-e2898701af1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n",
      "\n",
      "=== Interactive Sentiment Analysis ===\n",
      "Type 'quit' or 'exit' to end the program\n",
      "\n",
      "Enter text for sentiment analysis: i want to die\n",
      "\n",
      "Results:\n",
      "------------------------------\n",
      "Text: i want to die\n",
      "Sentiment: Negative\n",
      "Confidence: 85.88%\n",
      "------------------------------\n",
      "\n",
      "Enter text for sentiment analysis: i am enjoying here \n",
      "\n",
      "Results:\n",
      "------------------------------\n",
      "Text: i am enjoying here \n",
      "Sentiment: Positive\n",
      "Confidence: 99.02%\n",
      "------------------------------\n",
      "\n",
      "Enter text for sentiment analysis: i have ruined the party\n",
      "\n",
      "Results:\n",
      "------------------------------\n",
      "Text: i have ruined the party\n",
      "Sentiment: Negative\n",
      "Confidence: 98.77%\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6ac108ca102b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6ac108ca102b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Get user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0muser_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter text for sentiment analysis: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Check if the user wants to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def load_model_and_tokenizer(model_path, tokenizer_path):\n",
    "    # Load the trained model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # Load the tokenizer\n",
    "    with open(tokenizer_path, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def test_sentiment(model, tokenizer, text, max_len=150):\n",
    "    # Preprocess and predict\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')\n",
    "    prediction = float(model.predict(padded, verbose=0)[0][0])\n",
    "\n",
    "    sentiment = 'Positive' if prediction >= 0.5 else 'Negative'\n",
    "    confidence = prediction * 100 if prediction >= 0.5 else (1-prediction) * 100\n",
    "\n",
    "    return sentiment, confidence\n",
    "\n",
    "def main():\n",
    "    # Update these paths to your model and tokenizer locations\n",
    "    MODEL_PATH = '/content/drive/My Drive/Sentiment_Analysis/Models/best_model1.h5'\n",
    "    TOKENIZER_PATH = '/content/drive/My Drive/Sentiment_Analysis/Models/tokenizer.pickle'\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer = load_model_and_tokenizer(MODEL_PATH, TOKENIZER_PATH)\n",
    "    print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "    print(\"\\n=== Interactive Sentiment Analysis ===\")\n",
    "    print(\"Type 'quit' or 'exit' to end the program\")\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_text = input(\"\\nEnter text for sentiment analysis: \")\n",
    "\n",
    "        # Check if the user wants to exit\n",
    "        if user_text.lower() in ['quit', 'exit']:\n",
    "            print(\"Exiting program. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Analyze the sentiment of the user input\n",
    "        sentiment, confidence = test_sentiment(model, tokenizer, user_text)\n",
    "\n",
    "        # Display the results\n",
    "        print(\"\\nResults:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Text: {user_text}\")\n",
    "        print(f\"Sentiment: {sentiment}\")\n",
    "        print(f\"Confidence: {confidence:.2f}%\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUj8VYy8poyW"
   },
   "outputs": [],
   "source": [
    "##end "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP2PLIiXQ5h1+jCH+zqK6B1",
   "gpuType": "T4",
   "mount_file_id": "1uYliF5-aEgmyzumUZTj_0M8aqQeaBaX5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
